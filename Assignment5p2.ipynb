{"cells":[{"cell_type":"markdown","id":"7c1655df","metadata":{"id":"7c1655df"},"source":["# Week Five Part 2\n","\n","Ari and Lucas\n","\n","Our assignment for Week 5 is to use a dataset to predict the class of new documents as either spam and ham (non-spam) e-mails.\n","\n","We will be using the dataset provided for us:  UCI Machine Learning Repository: Spambase Data Set (https://archive.ics.uci.edu/dataset/94/spambase). Our first step is to load the data and do some prints on it just to see what we are working with."]},{"cell_type":"code","execution_count":11,"id":"4da5089e","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4da5089e","executionInfo":{"status":"ok","timestamp":1751927560934,"user_tz":240,"elapsed":575,"user":{"displayName":"Ariann C","userId":"03460121609660078047"}},"outputId":"7cd00bcf-342c-4f63-ceb2-0014ae958334"},"outputs":[{"output_type":"stream","name":"stdout","text":["   word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n","0            0.00               0.64           0.64           0.0   \n","1            0.21               0.28           0.50           0.0   \n","2            0.06               0.00           0.71           0.0   \n","3            0.00               0.00           0.00           0.0   \n","4            0.00               0.00           0.00           0.0   \n","\n","   word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n","0           0.32            0.00              0.00                0.00   \n","1           0.14            0.28              0.21                0.07   \n","2           1.23            0.19              0.19                0.12   \n","3           0.63            0.00              0.31                0.63   \n","4           0.63            0.00              0.31                0.63   \n","\n","   word_freq_order  word_freq_mail  ...  char_freq_;  char_freq_(  \\\n","0             0.00            0.00  ...         0.00        0.000   \n","1             0.00            0.94  ...         0.00        0.132   \n","2             0.64            0.25  ...         0.01        0.143   \n","3             0.31            0.63  ...         0.00        0.137   \n","4             0.31            0.63  ...         0.00        0.135   \n","\n","   char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n","0          0.0        0.778        0.000        0.000   \n","1          0.0        0.372        0.180        0.048   \n","2          0.0        0.276        0.184        0.010   \n","3          0.0        0.137        0.000        0.000   \n","4          0.0        0.135        0.000        0.000   \n","\n","   capital_run_length_average  capital_run_length_longest  \\\n","0                       3.756                          61   \n","1                       5.114                         101   \n","2                       9.821                         485   \n","3                       3.537                          40   \n","4                       3.537                          40   \n","\n","   capital_run_length_total  target  \n","0                       278       1  \n","1                      1028       1  \n","2                      2259       1  \n","3                       191       1  \n","4                       191       1  \n","\n","[5 rows x 58 columns]\n","Total emails/rows: 4601\n","Total spam emails (target=1): 1813\n","Total ham emails (target=0): 2788\n"]}],"source":["import pandas as pd\n","\n","#prepare column names\n","column_names = [\n","    'word_freq_make', 'word_freq_address', 'word_freq_all', 'word_freq_3d',\n","    'word_freq_our', 'word_freq_over', 'word_freq_remove', 'word_freq_internet',\n","    'word_freq_order', 'word_freq_mail', 'word_freq_receive', 'word_freq_will',\n","    'word_freq_people', 'word_freq_report', 'word_freq_addresses', 'word_freq_free',\n","    'word_freq_business', 'word_freq_email', 'word_freq_you', 'word_freq_credit',\n","    'word_freq_your', 'word_freq_font', 'word_freq_000', 'word_freq_money',\n","    'word_freq_hp', 'word_freq_hpl', 'word_freq_george', 'word_freq_650',\n","    'word_freq_lab', 'word_freq_labs', 'word_freq_telnet', 'word_freq_857',\n","    'word_freq_data', 'word_freq_415', 'word_freq_85', 'word_freq_technology',\n","    'word_freq_1999', 'word_freq_parts', 'word_freq_pm', 'word_freq_direct',\n","    'word_freq_cs', 'word_freq_meeting', 'word_freq_original', 'word_freq_project',\n","    'word_freq_re', 'word_freq_edu', 'word_freq_table', 'word_freq_conference',\n","    'char_freq_;', 'char_freq_(', 'char_freq_[', 'char_freq_!', 'char_freq_$',\n","    'char_freq_#', 'capital_run_length_average', 'capital_run_length_longest',\n","    'capital_run_length_total', 'target'\n","]\n","\n","#load data\n","url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\n","spambase = pd.read_csv(url, header=None, names=column_names)\n","\n","#prints\n","print(spambase.head())\n","print(f\"Total emails/rows: {len(spambase)}\")\n","print(\"Total spam emails (target=1):\", spambase['target'].sum())\n","print(\"Total ham emails (target=0):\", len(spambase) - spambase['target'].sum())"]},{"cell_type":"markdown","source":["There is 58 columns in spambase with a most of them holding relative frequencies of different words and characters. The couple extras are on capital letters as a lot of spam emails often have a lot of these i.e. URGENT NOTICE or LIMITED TIME OFFER. The last column 'target' is the binary column for spam (1) or ham (0).\n","\n","Next, we need to split our dataset into training and testing sets. We can see that the spambase has 4,601 rows of email info. A good split for training and testing sets is between 70:30 to 80:20. In this case, we will use 70:30."],"metadata":{"id":"twPJVRxnZWPu"},"id":"twPJVRxnZWPu"},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","# Split: 70% train, 30% test\n","X_train, X_test, y_train, y_test = train_test_split(spambase, spambase['target'], test_size=0.3, random_state=77)\n","\n","print(f\"Training set: {len(X_train)}\")\n","print(f\"Testing set: {len(X_test)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t1N8yZStZ_VO","executionInfo":{"status":"ok","timestamp":1751927948511,"user_tz":240,"elapsed":10,"user":{"displayName":"Ariann C","userId":"03460121609660078047"}},"outputId":"03aadfd7-3bcf-4b22-bb69-c7ece7f836ad"},"id":"t1N8yZStZ_VO","execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Training set: 3220\n","Testing set: 1381\n"]}]},{"cell_type":"markdown","source":["Next, we need to train a classifier for predicting if a email is spam or not. We chose Logistic Regression as our classifier because it is good with binary predictions and works well with numeric values which all our columns are.\n","\n","Below is a simple Logistic Regression classifier with just capital_run_length_total as our only predicting feature. This columns tell us the total number of capital letters in that email. As discussed above, a lot of spam emails tend to use capital letters to make their emails look urgent or pop out as the viewer."],"metadata":{"id":"yFgsAVqYaB9C"},"id":"yFgsAVqYaB9C"},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","#Logistic Regression model\n","model = LogisticRegression(max_iter=1000, random_state=42)\n","model.fit(X_train[['capital_run_length_total']], y_train)\n","\n","#use Logistic Regression model to make predictions on the test set\n","results = model.predict(X_test[['capital_run_length_total']])\n","\n","#print results\n","print(classification_report(y_test, results))\n","\n","accuracy = accuracy_score(y_test, results)\n","print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eKgvOhXPanRr","executionInfo":{"status":"ok","timestamp":1751929035099,"user_tz":240,"elapsed":140,"user":{"displayName":"Ariann C","userId":"03460121609660078047"}},"outputId":"bfd60ad8-28a0-4b4c-d6f7-a8e1d877c97c"},"id":"eKgvOhXPanRr","execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.63      0.92      0.75       803\n","           1       0.70      0.25      0.37       578\n","\n","    accuracy                           0.64      1381\n","   macro avg       0.66      0.59      0.56      1381\n","weighted avg       0.66      0.64      0.59      1381\n","\n","Accuracy: 64.08%\n"]}]},{"cell_type":"markdown","source":["The above simple Logistic Regression classifier got an accuracy score of 64.08% which is not that good considering this is a binary prediction so the average if we did this randomly with no features is 50%. This improve this we can add more features and we can standardize our features by scaling them after picking which to use.\n","\n","The features we added are:\n"],"metadata":{"id":"H7u-cut-aDLC"},"id":"H7u-cut-aDLC"},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\n","\n","#create list for the features as there is more than one now\n","X_train_feature = X_train[['capital_run_length_total']]\n","X_test_feature = X_test[['capital_run_length_total']]\n","\n","#scaling\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train_feature)\n","X_test_scaled = scaler.transform(X_test_feature)\n","\n","#Logistic Regression model\n","model = LogisticRegression(max_iter=1000, random_state=42)\n","model.fit(X_train_scaled, y_train)\n","\n","#use Logistic Regression model to make predictions on the test set\n","results = model.predict(X_test_scaled)\n","\n","#print results\n","print(classification_report(y_test, results))\n","\n","accuracy = accuracy_score(y_test, results)\n","print(f\"Accuracy: {accuracy * 100:.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"As8OrfpBanst","executionInfo":{"status":"ok","timestamp":1751931116231,"user_tz":240,"elapsed":26,"user":{"displayName":"Ariann C","userId":"03460121609660078047"}},"outputId":"04c974c3-6711-4f94-ebd1-7dce60c2ada2"},"id":"As8OrfpBanst","execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.63      0.93      0.75       803\n","           1       0.71      0.25      0.37       578\n","\n","    accuracy                           0.64      1381\n","   macro avg       0.67      0.59      0.56      1381\n","weighted avg       0.66      0.64      0.59      1381\n","\n","Accuracy: 64.23%\n"]}]},{"cell_type":"markdown","source":["With the new additional features and scaling, the accuracy went up from 64.08% to _%.\n","\n","Conclusion & Next steps"],"metadata":{"id":"xqBAjeN9aEY4"},"id":"xqBAjeN9aEY4"}],"metadata":{"kernelspec":{"display_name":"DATA624","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.13.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}